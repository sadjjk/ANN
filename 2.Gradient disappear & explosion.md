### 名词定义
![](https://upload-images.jianshu.io/upload_images/3290281-7637eed13f24934f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
靠近输出层的hidden layer梯度大，参数更新快，所以很快就会收敛；而靠近输入层的hidden layer 梯度小，参数更新慢，几乎就和初始状态一样，随机分布。这种现象叫梯度弥散，也叫**梯度消失**。**梯度爆炸**的情况正好相反 
### 原因解释
梯度消失的问题很大程度上是来源于**激活函数的“饱和”**。因为在后向传播的过程中仍然需要计算激活函数的导数，一旦落入激活函数的饱和区，它的梯度将变得非常小。使用反向传播算法传播梯度的时候，随着传播深度的增加，梯度的幅度会急剧减小，会导致浅层神经元的权重更新非常缓慢。而超过激活函数的饱和区，它的梯度将变的非常大，随着传播深度的增加，模型损失变成 NaN，即梯度爆炸。 
### 数学原理
假设三层隐藏层 激活函数为sigmoid函数
![](https://upload-images.jianshu.io/upload_images/3290281-7c60e8cd1f8ae72f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/3290281-e8bcb6b526411d1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/3290281-c6278f4c172644da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/3290281-e1652c3529e28a67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
sigmoid导数在x=0时取得最大值1/4。
如果我们使用均值为0，方差为1的高斯分布初始化参数w，有|w| < 1,所以有：![](https://upload-images.jianshu.io/upload_images/3290281-79d764cf16fe49c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
可以看出随着网络层数的加深，最后的乘积会指数级衰减，这就是梯度弥散的根本原因。
反之当|W|>4时，则wjf’(zj)>1，会使后面的layer参数指数级增加，从而引发梯度爆炸。
这种现象也叫***鸡汤*** 
![](https://upload-images.jianshu.io/upload_images/3290281-765eb626beb4ee54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
### 解决措施
- 使用非饱和激活函数代替饱和激活函数，如使用ReLU,maxout激活函数等替代sigmoid，tanh
- 梯度裁剪
- dropout和正则化
- CNN模型使用残差网络ResNet代替
- RNN模型使用LSTM代替
![](https://pic2.zhimg.com/80/v2-8d64e83943e31fb95af6b1845e174b49_hd.png)
LSTM相对普通RNN多了加和，为避免梯度消散提供了可能。线性自连接的memory是关键。
- batch normalization 批规范化 
每层激活函数之前，做均值、方差归一化、放大、缩小




